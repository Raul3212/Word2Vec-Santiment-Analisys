{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "import nbimporter\n",
    "from pre_processamento import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelName = \"Hobbit_cbow\"\n",
    "numFeatures = 300\n",
    "minWordCount = 1\n",
    "contextSize = 2\n",
    "model = False\n",
    "removeStopWords = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carrega dataset e realiza shuffle\n",
    "dataset = pd.read_csv(\"Data/Bases/HOBBIT.csv\", delimiter=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizador para separar texto em sentenças\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Instancia de PreProcessor para pre-processamento dos dados\n",
    "preprocessor_model = PreProcessor(remove_stopwords=False, word_min_count=minWordCount, tokenizer=tokenizer, stemming=True)\n",
    "\n",
    "# Set dataset e realiza pre-processamento\n",
    "preprocessor_model.fit(dataset[\"TEXT\"])\n",
    "\n",
    "# Pegando tweets após realização de pre-processamento\n",
    "cleaned_tweets_word_list = preprocessor_model.get_cleaned_sentences_word_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando modelo word2vec\n",
    "num_workers = 4  # Numero de threads que executarão em paralelo\n",
    "downsampling = 1e-3  # Downsample setting for frequent words\n",
    "model = word2vec.Word2Vec(cleaned_tweets_word_list, workers=num_workers, \\\n",
    "                          size=numFeatures, min_count=minWordCount, \\\n",
    "                          window=contextSize, sample=downsampling, sg=model)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"Data/ModelosW2V/{}_min{}_cont{}_cbow\".format(modelName, minWordCount, contextSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
